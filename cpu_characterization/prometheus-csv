#!/usr/bin/python

import csv
import requests
import sys
import datetime


# Represents a Prometheus server we can query
class Prometheus():
    def __init__(self, host, token):
        self.token = token
        self.host = host
        self._sanitize_host()

    def _sanitize_host(self):
        if self.host.startswith("https://"):
            self.host = self.host[8:]
        if self.host.startswith("api."):
            self.host = self.host[4:]
        if self.host.endswith(":6443"):
            self.host = self.host[:-5]
        if not self.host.startswith("prometheus-k8s-openshift-monitoring.apps"):
            self.host = f"prometheus-k8s-openshift-monitoring.apps.{self.host}"

    def api_for(self, api):
        return f"https://{self.host}/api/v1/{api}"

    def auth_header(self):
        return {'Authorization': f"Bearer {self.token}"}

    def query_time(self, time=None):
        if isinstance(time, datetime.datetime):
            return time
        if time is None:
            return datetime.datetime.utcnow() # <-- get time in UTC
        return datetime.datetime.fromisoformat(time)

    def query(self, query, time=None):
        params = {'query': query}
        time = self.query_time(time)
        params['time'] = time.isoformat("T") + "Z"
        response = requests.get(self.api_for('query'), headers=self.auth_header(), params=params, verify=False)
        if not response.ok:
            raise RuntimeError(f"Request failed: {response}")
        return [Prometheus.Metric(x) for x in response.json()['data']['result']]

    # Represents a metric result (mildly parsed)
    class Metric():
        def __init__(self, json):
            self.json = json
            self.name = json['metric']
            self.time = json['value'][0]
            self.value = json['value'][1]

        def __repr__(self):
            return f"{self.name}: {self.value} @{self.time}"


# Collect a set of metrics with a common function, interval, and time, combining them all into a single list.
def collect_metrics(prometheus, base_metrics, fn=None, interval=None, time=None):
    time = prometheus.query_time(time)
    result = []
    for metric in base_metrics:
        if fn is None:
            query = metric
        else:
            query = fn(metric, interval)
        result += prometheus.query(query, time=time)
    return result


# Given a dictionary of {fnname: [metric1, metric2]}, re-combobulate into {metric1.name: {fnname: metric1.value, fnname2: metric1.value, ...}}
def join_metrics(metric_sets):
    result = {}
    for setname, metrics in metric_sets.items():
        for m in metrics:
            metricname = repr(m.name)
            if metricname not in result:
                result[metricname] = {}
            result[metricname][setname] = m.value
    return result


# Collect a matrix of results for a common interval and time, and join them into a single dictionary of {metric-name: {fnname: value, fnname2: value2, ...}}
def multicollect(prometheus, metric_set, functions, interval=None, time=None):
    time = prometheus.query_time(time)
    return join_metrics({name: collect_metrics(prometheus, metric_set, fn=fn, interval=interval, time=time) for name, fn in functions.items()})


def filtered_metric(base_metric, *filters):
    result = base_metric
    validFilters = list(f for f in filters if f is not None and len(f) > 0)
    if len(validFilters) > 0:
        result += '{' + ",".join(validFilters) + '}'
    return result


def filter_out(labelname, *patterns):
    result = None
    validPatterns = list(p for p in patterns if p is not None and len(p) > 0)
    if len(validPatterns) > 0:
        result = f"{labelname}!~\"{'|'.join(validPatterns)}\""
    return result

host = None
token = None
if len(sys.argv) > 1:
    host = sys.argv[1]
if len(sys.argv) > 2:
    token = sys.argv[2]
interval = "1h"
if len(sys.argv) > 3:
    interval = sys.argv[3]
time = None
if len(sys.argv) > 4:
    time = sys.argv[4]
skip_namespaces = ()
if len(sys.argv) > 5:
    skip_namespaces = sys.argv[5:]
if not host and not token or host in ("-h", "--help"):
    print(f"Usage: {sys.argv[0]} host token [interval] [timestamp] [skip_namespaces...]")
    print("""
Compute the average, min, and max values over the given interval ending at the given timestamp, and return a CSV representation

If not specified, interval defaults to 1h and timestamp defaults to now

By default, skips any pod named "process-exp.*" to exclude the process-exporter itself
Adding additional skip_namespaces will also exclude any pods that match from the pod CPU usage accounting, so we can exclude workloads if required
""")
    sys.exit(1)

prometheus = Prometheus(host, token)
metric_set = [
    filtered_metric("namedprocess_namegroup_cpu_rate", filter_out("groupname", "conmon")),
    filtered_metric("pod:container_cpu_usage:sum", filter_out("podname", "process-exp.*"), filter_out("namespace", *skip_namespaces))
]
combined_metrics = multicollect(prometheus, metric_set, {
    f'avg over {interval}': lambda metric, interval: f"avg_over_time({metric}[{interval}])",
    f'min over {interval}': lambda metric, interval: f"min_over_time({metric}[{interval}])",
    f'max over {interval}': lambda metric, interval: f"max_over_time({metric}[{interval}])",
}, interval=interval, time=time)

labels = list(next(iter(combined_metrics.values())).keys())
writer = csv.writer(sys.stdout)
writer.writerow(['name'] + labels)
for name, values in combined_metrics.items():
    writer.writerow([name] + list(values[k] for k in labels))

